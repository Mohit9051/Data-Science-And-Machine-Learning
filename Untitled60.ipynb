{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fXYdyqiiTMUU",
        "outputId": "cf034f87-6f41-414f-f331-758976f92aa1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bd250d7-cf4c-49b1-a006-e6db90ebcfd4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bd250d7-cf4c-49b1-a006-e6db90ebcfd4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving convert.xlsx to convert.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel('convert.xlsx')  # Replace 'your_dataset.xlsx' with the actual path to your dataset\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    # Clean and format external status descriptions\n",
        "    df['externalStatus'] = df['externalStatus'].apply(lambda x: x.lower())\n",
        "\n",
        "    # Encode internal status labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['internalStatusEncoded'] = label_encoder.fit_transform(df['internalStatus'])\n",
        "\n",
        "    return df, label_encoder\n",
        "\n",
        "# Preprocess data\n",
        "df, label_encoder = preprocess_data(df)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['externalStatus'], df['internalStatusEncoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = max([len(seq) for seq in X_train_seq])\n",
        "\n",
        "# Padding sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "# Model Development using LSTM\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=max_len),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Model Evaluation\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Save tokenizer and label encoder\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Save model\n",
        "model.save('model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8vPyHmATpre",
        "outputId": "0e421793-f771-4f59-8734-e11d7db6ec61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 5s 119ms/step - loss: 2.4616 - accuracy: 0.2685 - val_loss: 2.1323 - val_accuracy: 0.2857\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 2.0533 - accuracy: 0.3072 - val_loss: 1.9612 - val_accuracy: 0.2857\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 1s 85ms/step - loss: 1.8292 - accuracy: 0.4505 - val_loss: 1.6362 - val_accuracy: 0.5204\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 1s 89ms/step - loss: 1.2952 - accuracy: 0.5927 - val_loss: 0.9905 - val_accuracy: 0.7959\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 0.7654 - accuracy: 0.8020 - val_loss: 0.5734 - val_accuracy: 0.7959\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 1s 52ms/step - loss: 0.5190 - accuracy: 0.8487 - val_loss: 0.4215 - val_accuracy: 0.8878\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 1s 54ms/step - loss: 0.3970 - accuracy: 0.9022 - val_loss: 0.3774 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 1s 64ms/step - loss: 0.2942 - accuracy: 0.9306 - val_loss: 0.2041 - val_accuracy: 0.9490\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 1s 54ms/step - loss: 0.1703 - accuracy: 0.9568 - val_loss: 0.1408 - val_accuracy: 0.9694\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 1s 52ms/step - loss: 0.1152 - accuracy: 0.9818 - val_loss: 0.1038 - val_accuracy: 0.9694\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1687 - accuracy: 0.9633\n",
            "Test Loss: 0.1687040477991104\n",
            "Test Accuracy: 0.9632652997970581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "# Get the filename of the current script\n",
        "file_name = inspect.getframeinfo(inspect.currentframe()).filename\n",
        "\n",
        "# Extract just the filename from the full path\n",
        "file_name = os.path.basename(file_name)\n",
        "\n",
        "print(\"The name of the Python file containing the API code is:\", file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP6luob7XiHL",
        "outputId": "9aba653d-5b10-44c2-cc2f-5455e06a3929"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name of the Python file containing the API code is: <ipython-input-13-fe1b572491cc>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6C5d5XiapNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load tokenizer, label encoder, and model\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# API Initialization\n",
        "app = FastAPI()\n",
        "\n",
        "# API Input Data Model\n",
        "class InputData(BaseModel):\n",
        "    externalStatus: str\n",
        "\n",
        "# API Route for Predictions\n",
        "@app.post(\"/predict\")\n",
        "async def predict(data: InputData):\n",
        "    # Tokenization\n",
        "    seq = tokenizer.texts_to_sequences([data.externalStatus.lower()])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Model Prediction\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label_idx = np.argmax(prediction)\n",
        "\n",
        "    # Decode predicted label\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
        "\n",
        "    return {\"predicted_internal_status\": predicted_label}\n"
      ],
      "metadata": {
        "id": "31tbg9XdXnMh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn \"<ipython-input-13-fe1b572491cc>:app\" --reload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuMZO9LVbFuQ",
        "outputId": "30ec54ee-20fc-4be4-ecf1-7fa70c5d6557"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m9083\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"<ipython-input-13-fe1b572491cc>\".\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m9083\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn tensorflow pandas openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7eVxxQRbUri",
        "outputId": "fcd8912c-4f0c-41d6-986e-42f7225bf649"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.110.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.4)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.36.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.10.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi) (3.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn ipython-input-13-fe1b572491cc:app --reload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HirvjeWaX1bc",
        "outputId": "2f936fa1-ba14-4589-81e6-7d4f037da77d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m5973\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"ipython-input-13-fe1b572491cc\".\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m5973\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWrkroacYiUu",
        "outputId": "bf1ba8cf-5bb2-4fae-d517-446bcd9cba70"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn app:app --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knB6G2NtYqqI",
        "outputId": "34b6599d-733c-480b-b8c0-52b412654478"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"app\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inside app.py\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Define your routes and other app configurations here\n"
      ],
      "metadata": {
        "id": "1Nh7ySRfY1wX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP0OHTlfUIDI",
        "outputId": "8eddf28a-26a2-445a-81b6-665de946bf0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.4)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.2.0)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.110.0 starlette-0.36.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load tokenizer, label encoder, and model\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# API Initialization\n",
        "app = FastAPI()\n",
        "\n",
        "# API Input Data Model\n",
        "class InputData(BaseModel):\n",
        "    externalStatus: str\n",
        "\n",
        "# API Route for Predictions\n",
        "@app.post(\"/predict\")\n",
        "async def predict(data: InputData):\n",
        "    # Tokenization\n",
        "    seq = tokenizer.texts_to_sequences([data.externalStatus.lower()])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Model Prediction\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label_idx = np.argmax(prediction)\n",
        "\n",
        "    # Decode predicted label\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
        "\n",
        "    return {\"predicted_internal_status\": predicted_label}\n"
      ],
      "metadata": {
        "id": "I49TISMVURXB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset from Excel file\n",
        "df = pd.read_excel(\"convert.xlsx\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Preprocess Data\n",
        "# Assuming the columns are named 'externalStatus' and 'internalStatus'\n",
        "# You may need to adjust column names if they are different in your dataset\n",
        "\n",
        "# Remove any leading/trailing whitespaces\n",
        "df['externalStatus'] = df['externalStatus'].str.strip()\n",
        "df['internalStatus'] = df['internalStatus'].str.strip()\n",
        "\n",
        "# Encode categorical variables into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "df['externalStatus_encoded'] = label_encoder.fit_transform(df['externalStatus'])\n",
        "df['internalStatus_encoded'] = label_encoder.fit_transform(df['internalStatus'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = df['externalStatus_encoded']\n",
        "y = df['internalStatus_encoded']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optionally, you may want to save the preprocessed data to use later\n",
        "# df.to_excel(\"preprocessed_data.xlsx\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7up0aIc27Y",
        "outputId": "df007b13-ee74-4b24-be32-3d4f1b8261bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      externalStatus    internalStatus\n",
            "0                                           PORT OUT          Port Out\n",
            "1                                        TERMINAL IN  Inbound Terminal\n",
            "2                                            PORT IN           Port In\n",
            "3  Vessel departure from first POL (Vessel name :...         Departure\n",
            "4  Vessel arrival at final POD (Vessel name : TIA...           Arrival\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(1,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6mNVxcDc7cF",
        "outputId": "deccc36e-b3e2-44e9-c363-9222f8f4a2e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 64)                128       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4353 (17.00 KB)\n",
            "Trainable params: 4353 (17.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI_MMvPPdJaX",
        "outputId": "a723dc28-456b-4da8-a151-c91cd27a75ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31/31 [==============================] - 2s 10ms/step - loss: -145.8119 - accuracy: 0.2313 - val_loss: -263.9944 - val_accuracy: 0.2449\n",
            "Epoch 2/10\n",
            "31/31 [==============================] - 0s 3ms/step - loss: -408.3460 - accuracy: 0.2323 - val_loss: -619.4023 - val_accuracy: 0.2449\n",
            "Epoch 3/10\n",
            "31/31 [==============================] - 0s 3ms/step - loss: -911.2420 - accuracy: 0.2323 - val_loss: -1303.9830 - val_accuracy: 0.2449\n",
            "Epoch 4/10\n",
            "31/31 [==============================] - 0s 3ms/step - loss: -1834.2446 - accuracy: 0.2323 - val_loss: -2527.9783 - val_accuracy: 0.2449\n",
            "Epoch 5/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: -3383.3181 - accuracy: 0.2323 - val_loss: -4500.3525 - val_accuracy: 0.2449\n",
            "Epoch 6/10\n",
            "31/31 [==============================] - 0s 5ms/step - loss: -5801.2041 - accuracy: 0.2323 - val_loss: -7402.0469 - val_accuracy: 0.2449\n",
            "Epoch 7/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: -9230.6094 - accuracy: 0.2323 - val_loss: -11556.4736 - val_accuracy: 0.2449\n",
            "Epoch 8/10\n",
            "31/31 [==============================] - 0s 5ms/step - loss: -13992.0576 - accuracy: 0.2323 - val_loss: -17012.5430 - val_accuracy: 0.2449\n",
            "Epoch 9/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: -20228.1543 - accuracy: 0.2323 - val_loss: -24074.9277 - val_accuracy: 0.2449\n",
            "Epoch 10/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: -28124.4512 - accuracy: 0.2323 - val_loss: -33128.8359 - val_accuracy: 0.2449\n",
            "8/8 [==============================] - 0s 2ms/step - loss: -33128.8359 - accuracy: 0.2449\n",
            "Test Loss: -33128.8359375, Test Accuracy: 0.2448979616165161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define request and response models\n",
        "class StatusPredictionRequest(BaseModel):\n",
        "    external_status: str\n",
        "\n",
        "class StatusPredictionResponse(BaseModel):\n",
        "    internal_status: str\n",
        "\n",
        "# Define API endpoint\n",
        "@app.post(\"/predict_status/\")\n",
        "def predict_status(request: StatusPredictionRequest):\n",
        "    # Preprocess the input\n",
        "    encoded_status = label_encoder.transform([request.external_status.strip()])\n",
        "\n",
        "    # Make prediction using the trained model\n",
        "    prediction = model.predict(encoded_status)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    predicted_internal_status = label_encoder.inverse_transform([round(prediction[0][0])])\n",
        "\n",
        "    return {\"internal_status\": predicted_internal_status[0]}\n"
      ],
      "metadata": {
        "id": "6o889gO3dTXn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "# Get the filename of the current script\n",
        "file_name = inspect.getframeinfo(inspect.currentframe()).filename\n",
        "\n",
        "# Extract just the filename from the full path\n",
        "file_name = os.path.basename(file_name)\n",
        "\n",
        "print(\"The name of the Python file containing the API code is:\", file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yAVJ0GWgHvZ",
        "outputId": "d5d52840-3fd2-4897-9277-456725696588"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name of the Python file containing the API code is: <ipython-input-52-fe1b572491cc>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# List all the files in the current directory\n",
        "files = os.listdir(cwd)\n",
        "\n",
        "# Print the list of files to find your script name\n",
        "print(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbbdARYQdzLf",
        "outputId": "f1bca177-1f50-40d6-8651-926ac302a69b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'label_encoder.pkl', 'tokenizer.pkl', 'model.h5', 'convert.xlsx', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:\n",
        "\n",
        "Read the dataset from an Excel file.\n",
        "Clean the dataset by removing unnecessary columns and handling missing values.\n",
        "Encode categorical variables using techniques like Label Encoding or One-Hot Encoding.\n",
        "Model Development:\n",
        "\n",
        "Split the preprocessed dataset into features (input) and target variable (output).\n",
        "Choose an appropriate machine learning or deep learning algorithm for the task.\n",
        "Design the model architecture, including the number and types of layers.\n",
        "Compile the model with a suitable loss function, optimizer, and evaluation metric.\n",
        "Model Training and Evaluation:\n",
        "\n",
        "Train the model on the training dataset using the fit function.\n",
        "Monitor the training process and evaluate the model's performance on a separate testing dataset using the evaluate function.\n",
        "Experiment with different architectures, hyperparameters, and algorithms to achieve optimal performance.\n",
        "API Development:\n",
        "\n",
        "Use a framework like FastAPI to develop an API for exposing the trained model.\n",
        "Define API endpoints to accept input data (external status descriptions) and return predicted internal status labels.\n",
        "Implement error handling and input validation within the API.\n",
        "Testing and Validation:\n",
        "\n",
        "Thoroughly test the developed API to ensure its functionality and accuracy.\n",
        "Validate the predictions made by the model against a validation dataset to assess the model's generalization ability.\n",
        "Monitor the API's performance and responsiveness under various scenarios and workloads.\n",
        "Documentation:\n",
        "\n",
        "Document each step of the project, including data preprocessing, model development, training and evaluation, API development, and testing.\n",
        "Provide clear explanations, comments, and docstrings in the code to enhance readability and maintainability.\n",
        "Include information on the dataset, model architecture, hyperparameters, and API endpoints in the documentation.\n",
        "Consider creating a README file with instructions for running the code, setting up the environment, and using the API.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6xrZtdoUipei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMICEh1Xirgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}